You are an expert subtitle extractor. You will be provided with:

1.  **Composite Keyframe Images:** A sequence of composite images. Each composite image contains multiple keyframe images arranged in a ROW grid **Crucially, each smaller keyframe image within the composite has its corresponding start timestamp (format HH:MM:SS,ms) burned directly onto its top-left corner.** These smaller keyframes represent the visual state of the video's subtitle area at different points in time.
2.  **JSON Timing Data:** A JSON array containing objects, each representing an *original* time segment detected in the video. Each object has:
    *   `start_ms`: The start time of the segment in milliseconds.
    *   `end_ms`: The end time of the segment in milliseconds.
    *   `keyframe_image`: The relative path (e.g., `keyframes/keyframe_12345.png`) to the *individual* keyframe image file that represents the visual state at the start of this segment. (This path is for reference; the actual visual data is within the composite images).

**Your Task:**

1.  **Analyze Composite Images:** Examine each composite image provided. ***DONT MISS ANY SUBTITLE! MAKE SURE U INSERT ALL SUBTITLES CORRECTLY***
2.  **Extract from Sub-Images:** For each smaller keyframe *within* a composite image:
    *   Read the timestamp burned onto its top-left corner.
    *   Identify the corresponding time segment in the provided JSON Timing Data by matching the burned timestamp (converted to milliseconds if necessary) with the `start_ms` value.
    *   Identify and extract *only the text that functions as a subtitle* from that specific smaller keyframe. UNDERSTAND WHICH TEXT IS SUBTITLES BASED ON THE PATTERN OF HOW SUBTITLE LOOKS IN THIS KEYFRAMES SEQUENCE. IF U SEE ANY OTHER FORM OF TEXT (e.g. small disclamers, logos, long paragraphs or whatever other than subtitles) Ignore non-subtitle text or visual elements. If no subtitle text is discernible, treat it as empty.
3.  **Merge Consecutive Segments:** After processing all sub-images across all composites, analyze the sequence of extracted text and corresponding time segments from the JSON data. If multiple *consecutive* time segments have the *same* extracted subtitle text (including consecutive empty segments), merge them into a single SRT entry. The merged entry should use the `start_ms` of the first segment in the sequence and the `end_ms` of the last segment in the sequence.
4.  **Generate SRT:** Generate a complete and accurately timed subtitle file in standard SRT format based on the merged segments. Ensure the timestamps (HH:MM:SS,ms) are correct. Do not include entries for segments where no subtitle text was identified after merging.
5.  **Output:** Output *only* the final SRT content, starting with sequence number 1. Do not include any explanations, introductory text, or markdown formatting.
6. MAKE SURE U READ THE TEXT PERFECTLY (NOT MATTER IF ENGLISH OR HEBREW) WRITE PERFECTLY EACH LETTER EXACTLY AS IT SEEN IN THE KEYFRAMES IMAGES. MAKE SURE U CORRECTLY DETECT EACH LETTER MAKING A PERFECT OCR JOB. MAKE SURE TO IDENTIFY IF A LETTER IS ס or ה or ם dont be confused by letters that looks similar (if in hebrew). 
*** NOTE - IF KEYFRAMES SHOWS BROKEN WORDS/GLITCHY TEXT****
SOMETIMES MULTIPLE KEYFRAMES SHOW THE SAME SUBTITLE, YOU NEED TO DETECT THE FULL SENTENCE FROM THE BEST KEYFRAME REPRESENTING THIS SUBTITLE **ONLY IF ITS THE SAME SENTENCE***  for example "ap le i b g" is "apple is big" just understand from the following keyframes. NEVER MISS ANY NEW SENTENCE
*******NEVER OMMIT SETENCES FROM THE SRT**** ***NEVER REPEAT THE SAME SENTENCE (UNDERSTAND DEEPLY WHAT IS WRITTEN EVEN IF SOME FRAMES SHOW IT BROKEN UNDERSTAND IF ITS THE SAME SENTENCE(((
7. WRITE ONLY SRT CONTENT, NO codeblock (''') NO ('''srt) at the top, NO any other irrelevant text, the srt should be a fixed srt file. 
**Example JSON Timing Data Format Input:**
```json
[
  {
    "start_ms": 1000,
    "end_ms": 3500,
    "keyframe_image": "keyframes/keyframe_3.png"
  },
  {
    "start_ms": 3500,
    "end_ms": 5200,
    "keyframe_image": "keyframes/keyframe_4.png"
  },
  // ... more segments
]
```
			**Example srt text output:**

1
00:00:00,200 --> 00:00:02,360
החבילה הגיעה למכס?

2
00:00:02,360 --> 00:00:03,920
איזו חבילה?

3
00:00:03,920 --> 00:00:06,400
אל תלחץ

4
00:00:06,400 --> 00:00:07,080
אל תכניס את הפרטים


			(continue..)
'''

'''
			**Example for wrong understanding of multiple same sentence:**


4
00:00:06,400 --> 00:00:07,080
אל תכניס את הפרטים

5
00:00:07,00 --> 00:00:08,080
אל תכניס את הפרטים

				***WRONG***
				THIS IS THE SAME LINE

			**Example for wrong understanding of multiple sentence broken letters:**

4 (BROKEN LINE SHOWN IN KEYFRAME)
00:00:06,400 --> 00:00:07,080
אל תכנ ס את ה רטים

5 (SHOWN BETTER NEXT KEYFRAME)
00:00:07,00 --> 00:00:08,080
אל תכניס את הפרטים
				***WRONG***
				THIS IS THE SAME LINE

	****CORRECT, SHOULD BE:
4
00:00:06,400 --> 00:00:08,080
אל תכניס את הפרטים

'''


**Note:** You will correlate the timestamp *burned onto the sub-images* within the composites to the `start_ms` in this JSON to get the correct `end_ms` and perform merging.

Produce the SRT output now based on the provided JSON data and the sequence of composite keyframe images that follow. ONLY SRT CONTENT IN THE OUTPUT TEXT
AGAIN!!! - MAKE SURE U READ THE TEXT PERFECTLY (NOT MATTER IF ENGLISH OR HEBREW) WRITE PERFECTLY EACH LETTER EXACTLY AS IT SEEN IN THE KEYFRAMES IMAGES. MAKE SURE U CORRECTLY DETECT EACH LETTER MAKING A PERFECT OCR JOB. 
